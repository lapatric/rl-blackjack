{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: MIT License\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing an action\n",
    "The *state* comprises of a 3-tuple (int, int, bool): [0] you value, [1] the value of the dealer's face-up card and [2] a boolean whether the player holds a usable ace (an ace that can count as 11 without busting). There are two *actions*: (0) Stand and (1) hit.\n",
    "\n",
    "We execute the action in our environment and receive data from the environment\n",
    "- `next_state`: This is the observation that the agent will receive after taking the action.\n",
    "- `reward`: This is the reward that the agent will receive after taking the action.\n",
    "- `terminated`: This is a boolean variable that indicates whether or not the environment has terminated.\n",
    "- `truncated`: This is a boolean variable that also indicates whether the episode ended by early truncation, i.e., time limit is reached.\n",
    "- `info`: This is a dictionary that might contain additional information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Value: \t 21 \t 21\n",
      "Dealer Value: \t 9 \t 9\n",
      "Usable Ace: \t True \t False\n",
      "Sampled action:  Hit\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "\n",
    "action = env.action_space.sample()\n",
    "\n",
    "next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(\"My Value: \\t\", state[0], \"\\t\", next_state[0])\n",
    "print(\"Dealer Value: \\t\", state[1], \"\\t\", next_state[1])\n",
    "print(\"Usable Ace: \\t\", state[2], \"\\t\", next_state[2])\n",
    "print(\"Sampled action: \", \"Hit\" if action else \"Stand\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackjackAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a Reinforcement Learning agent with an empty dictionary of state-action values (q_values),\n",
    "        a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "        - learning_rate: Amount with which to weight newly learned reward vs old reward (1 - lr)\n",
    "        - initial epsilon: The initial probability w/ with we sample random action (exploration)\n",
    "        - epsilon_decay: Value by which epsilon value decays through subtraction\n",
    "        - final_epsilon: Epsilon value at which decay stops\n",
    "        - discount_factor: The factor by which future rewards are counted, i.e. expected return on next state (recursive)\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        \n",
    "        self.training_error = []\n",
    "    \n",
    "    def get_action(self, state: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon) -> exploitation. \n",
    "        Otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[state]))\n",
    "    \n",
    "    def update(\n",
    "        self,\n",
    "        state: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_state: tuple[int, int, bool]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Updates the Q-value of an action.\n",
    "        The Q-value update is equivalent to the following weighting of old and new information by the learning rate:\n",
    "        # self.q_values[state][action] = (1 - self.lr) * self.q_values[state][action] +\n",
    "        #                                self.lr * (reward + self.discount_factor * future_q_value)\n",
    "        The temporal difference is the difference between the old and new value over one (time) step.\n",
    "        \"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_state]) \n",
    "        temporal_difference = reward + self.discount_factor * future_q_value - self.q_values[state][action]\n",
    "        self.q_values[state][action] = self.q_values[state][action] + self.lr * temporal_difference\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing training loop\n",
    "\n",
    "Once `terminated` or `truncated` is True, we stop the current episode and begin a new one with `env.reset()`. \n",
    "If we continue executing actions without resetting the environment, it still respond but the output won’t be useful \n",
    "for training (it might even be harmful if the agent learns on invalid data)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once terminated or truncated is True, we should stop the current episode and begin a new one with env.reset(). \n",
    "# If you continue executing actions without resetting the environment, it still respond but the output won’t be useful \n",
    "# for training (it might even be harmful if the agent learns on invalid data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate = learning_rate,\n",
    "    initial_epsilon = start_epsilon,\n",
    "    epsilon_decay = epsilon_decay,\n",
    "    final_epsilon = final_epsilon,\n",
    ")\n",
    "\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    #play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(state, action, reward, terminated, next_state)\n",
    "\n",
    "        # update done status and state\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "\n",
    "    # once a game is finished we decay epsilon -> converge towards exploitation\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "\n",
    "# Episode rewards plot\n",
    "axs[0].set_title(\"Episode rewards\")\n",
    "reward_moving_average = (\n",
    "    np.convolve(np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\") / rolling_length\n",
    ")\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "\n",
    "# Episode lengths plot\n",
    "axs[1].set_title(\"Episode lengths\")\n",
    "length_moving_average = (\n",
    "    np.convolve(np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\") / rolling_length\n",
    ")\n",
    "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "\n",
    "# Training error plot\n",
    "axs[2].set_title(\"Training Error\")\n",
    "training_error_moving_average = (\n",
    "    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\") / rolling_length\n",
    ")\n",
    "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28c168c855826d6a306c7ef584e2992a063631e8777795e1e4ec8bd8ff15ed8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
